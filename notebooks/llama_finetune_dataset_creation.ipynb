{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import objectify\n",
    "from xml.etree import ElementTree as ET\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from geo_llama.data import GeoVirusArticle, LGLArticle, WikTorArticle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a fine-tuning dataset for Llama-3 Geoparser fine tuning\n",
    "We will construct a dataset using the LgL and GeoVirus datasets with which we will fine-tune a custom Llama-3 model. The model will be tested on the News2024 dataset to assess the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geollama_prompt =\"\"\"Below is an instruction that describes a task, paired with an input that provides a specfic example which the task should be applied to. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "geoparse_instruction = \"\"\"Extract all toponyms from the provided text and estimate their geolocations. Include the name of every toponym in the text and its decimal latitude and longitude coordinates. Do not consider ajdectives (e.g. 'English', 'Iranian') as toponyms. Format the output in JSON, strictly adhering to the specified template. Be very concise and output only the JSON data inside a code block. Do not provide any explanation or reasoning.\n",
    "\n",
    "JSON Template for output:\n",
    "\n",
    "{\"toponyms\": [\n",
    "        {\n",
    "          \"name\": \"<string : toponym name exactly as it appears in the text>\",\n",
    "          \"latitude\": <float : latitude in decimal degrees>,\n",
    "          \"longitude\": <float : longitude in decimal degrees>\n",
    "        },\n",
    "        // More toponyms from the text can follow\n",
    "      ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LGL data\n",
    "# open the lgl dataset using xml\n",
    "dataset = 'lgl'\n",
    "\n",
    "def get_data(dataset):\n",
    "    xml = ET.parse(f\"../data/fine_tuning_data/{dataset}.xml\")\n",
    "    xml_root = xml.getroot()\n",
    "\n",
    "    xml_str = ET.tostring(xml_root,method='xml').decode()\n",
    "    xml_obj = objectify.fromstring(xml_str)\n",
    "    return xml_obj\n",
    "\n",
    "def build_ft_data(xml_obj, dataset):\n",
    "    \n",
    "    ft_data = []\n",
    "    if dataset in ['lgl', 'GeoVirus']:\n",
    "        articles = xml_obj.article\n",
    "    elif dataset in ['WikToR']:\n",
    "        articles = xml_obj.page\n",
    "    for article_xml in articles:\n",
    "        if dataset=='lgl':\n",
    "            article = LGLArticle(article_xml)\n",
    "        elif dataset=='GeoVirus':\n",
    "            article = GeoVirusArticle(article_xml)\n",
    "        elif dataset=='WikToR':\n",
    "            article = WikTorArticle(article_xml)\n",
    "        text = article.text\n",
    "        response = {\"toponyms\":[]}\n",
    "        for toponym in article.toponyms:\n",
    "            try:\n",
    "                response['toponyms'].append({\"name\":str(toponym.phrase),\n",
    "                                            \"latitude\":float(toponym.latitude),\n",
    "                                            \"longitude\":float(toponym.longitude)})\n",
    "            except:\n",
    "                response['toponyms'].append({\"name\":str(toponym.phrase),\n",
    "                                             \"latitude\":None,\n",
    "                                             \"longitude\":None})\n",
    "        ft_data.append({\"instruction\":geoparse_instruction,\n",
    "                        \"input\":str(text),\n",
    "                        \"response\":response})\n",
    "    \n",
    "    return ft_data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgl_xml = get_data('lgl')\n",
    "geovirus_xml = get_data('GeoVirus')\n",
    "\n",
    "lgl_ft_data = build_ft_data(lgl_xml, 'lgl')\n",
    "geovirus_ft_data = build_ft_data(geovirus_xml, 'GeoVirus')\n",
    "\n",
    "ft_data = lgl_ft_data + geovirus_ft_data\n",
    "\n",
    "with open('../data/fine_tuning_data/llama3_ft_data.json', 'w') as f:\n",
    "    json.dump(ft_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_data('TR-News')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a fine-tuning dataset for RAG based Llama-3 Geoparser\n",
    "We will construct a dataset using the LgL and GeoVirus datasets with which we will fine-tune a custom Llama-3 model. The model will be tested on the News2024 dataset to assess the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgl_xml = get_data('lgl')\n",
    "geovirus_xml = get_data('GeoVirus')\n",
    "trnews_xml = get_data('TR-News')\n",
    "\n",
    "ft_articles = []\n",
    "for article_xml in lgl_xml.article:\n",
    "    ft_articles.append(LGLArticle(article_xml))\n",
    "for article_xml in geovirus_xml.article:\n",
    "    ft_articles.append(GeoVirusArticle(article_xml))\n",
    "for article_xml in trnews_xml.article:\n",
    "    ft_articles.append(LGLArticle(article_xml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides a specfic example which the task should be applied to. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoparse_instruction = \"\"\"You will be given a piece of text, a toponym found within that text, and a JSON detailing the matched locations when that toponym is searched on OpenStreetMaps. \n",
    "\n",
    "Your task is to identify the matched location which is most likely to be the true location of the toponym, given the context of the text.\n",
    "\n",
    "If the list of matches is empty, or you do not think any match accurately represents the toponym, you are permitted to assign your best estimate for a latitude and longitude. This should be highlighted in your response by setting {\"RAG\":false}.\n",
    "\n",
    "Your output should strictly conform to the following tmeplate:\n",
    "\n",
    "{\"name\" : <(str) name of toponym as it appears in the text>,\n",
    " \"latitude\": <(float) latitude as it appears in the matched locations>,\n",
    " \"longitude\": <(float) longitude as it appears in the matched locations>,\n",
    " \"RAG_estimated\": <(bool) true if a matched location was used>\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "input_prompt = r\"\"\"<text> {} <\\text>\n",
    "\n",
    "<toponym> {} <\\toponym>\n",
    "\n",
    "<matches> {} <\\matches>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fine_tuning_data/nominatim_cache.json', 'r') as f:\n",
    "    cache = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import distance\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "from geo_llama.gazetteer import Gazetteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominatim = Gazetteer(gazetteer_source='nominatim', polygon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(toponym, cache):\n",
    "    try:\n",
    "        return cache[toponym], cache\n",
    "    except KeyError:\n",
    "        user_agent = f'GeoLlama_{random.uniform(1000,10000)}'\n",
    "        matches = nominatim.query(toponym, user_agent)\n",
    "        cache[toponym] = matches\n",
    "        return matches, cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 935/935 [01:51<00:00,  8.36it/s] \n"
     ]
    }
   ],
   "source": [
    "ft_data = []\n",
    "\n",
    "for article in tqdm(ft_articles):\n",
    "    \n",
    "    for toponym in article.toponyms:\n",
    "        if not toponym.latitude:\n",
    "            continue\n",
    "        true_point = (float(toponym.latitude), float(toponym.longitude))\n",
    "        matches, cache = get_matches(str(toponym.phrase), cache)\n",
    "        best_match = None\n",
    "        best_d = np.inf\n",
    "        for match in matches:\n",
    "            match_point = (float(match['lat']), float(match['lon']))\n",
    "            d = distance.distance(match_point, true_point)\n",
    "            if d < best_d:\n",
    "                best_match = match\n",
    "                best_d = d\n",
    "        # check if any match was very good:\n",
    "        if len(matches)==0:\n",
    "            response = {'name':toponym.phrase,\n",
    "                        'latitude':toponym.latitude,\n",
    "                        'longitude':toponym.longitude,\n",
    "                        'RAG_estimated':False}\n",
    "            \n",
    "        elif (best_d.km > 20) and (best_match['addresstype'] not in ['country', 'state', 'county', 'region']):\n",
    "            response = {'name':toponym.phrase,\n",
    "                        'latitude':toponym.latitude,\n",
    "                        'longitude':toponym.longitude,\n",
    "                        'RAG_estimated':False}\n",
    "        else:\n",
    "            response = {'name':toponym.phrase,\n",
    "                        'latitude':best_match['lat'],\n",
    "                        'longitude':best_match['lon'],\n",
    "                        'RAG_estimated':True}\n",
    "    \n",
    "        match_info = [{'name':m['name'], 'lat':m['lat'], 'lon':m['lon'], 'address':m['display_name']} for m in matches]\n",
    "        input = input_prompt.format(article.text, toponym.phrase, match_info)\n",
    "        \n",
    "        ft_prompt = RAG_prompt.format(geoparse_instruction, input, response)\n",
    "        ft_data.append({\"instruction\":geoparse_instruction,\n",
    "                        \"input\":input,\n",
    "                        \"response\":str(response)})\n",
    "               \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4682"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deduplicate\n",
    "ft_data = [dict(t) for t in {tuple(d.items()) for d in ft_data}]\n",
    "\n",
    "len(ft_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fine_tuning_data/llama3_RAG_geoparsing_ft_new.json', 'w') as f:\n",
    "    json.dump(ft_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a fine-tuning dataset for Llama-3 toponym extraction\n",
    "We will construct a dataset using the LgL and GeoVirus datasets with which we will fine-tune a custom Llama-3 model. The model will be tested on the News2024 dataset to assess the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoparse_instruction = \"\"\"You will be given a piece of text which contains some place names (toponyms). Please extract each toponyhm from the text and place it in a python list.\n",
    "\n",
    "Each toponym should only appear once in the list, even if they occur multiple times in the text. If multiple spellings of the same toponym appear in the text each spelling should be represented in the list.\n",
    "\n",
    "You should not consider adjectives (e.g. \"English\", \"Iranian\") as toponyms. Some toponyms may span multiple words.\n",
    "\n",
    "Please use the following template to structure your response:\n",
    "\n",
    "{\"toponyms\":[\"toponym_1\", \"toponym_2\", \"toponym_3\",...]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data = []\n",
    "\n",
    "for article in tqdm(ft_articles):\n",
    "    \n",
    "    toponyms = [str(t.phrase) for t in article.toponyms]\n",
    "    response = {\"toponyms\":list(set(toponyms))}\n",
    "    input = article.text\n",
    "    ft_data.append({\"instruction\":geoparse_instruction,\n",
    "                    \"input\":input,\n",
    "                    \"response\":str(response)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fine_tuning_data/llama3_toponym_extraction_ft.json', 'w') as f:\n",
    "    json.dump(ft_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the CoNLL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_conll2003_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def process_conll2003_data(lines):\n",
    "    dataset = []\n",
    "    article_text = []\n",
    "    toponyms = []\n",
    "    current_location = []\n",
    "    text_id = -1\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line == \"-DOCSTART- -X- -X- O\":\n",
    "            # Save the previous article if it exists\n",
    "            if article_text:\n",
    "                if current_location:\n",
    "                    toponyms.append(\" \".join(current_location))\n",
    "                dataset.append({\n",
    "                    'text_id': text_id,\n",
    "                    'text': \"\".join(article_text),\n",
    "                    'toponyms': toponyms\n",
    "                })\n",
    "                # Reset for the new article\n",
    "                article_text = []\n",
    "                toponyms = []\n",
    "                current_location = []\n",
    "            text_id += 1\n",
    "            continue\n",
    "        \n",
    "        if line == \"\":\n",
    "            # Add a blank line to the article text to maintain formatting\n",
    "            article_text.append(\"\\n\")\n",
    "            continue\n",
    "        \n",
    "        token, pos, chunk, ner = line.split()\n",
    "\n",
    "        # Handle apostrophes and split words\n",
    "        if len(article_text) > 0 and (token.startswith(\"'\") or token.startswith(\"-\")):\n",
    "            article_text[-1] += token\n",
    "        else:\n",
    "            # Handle spacing for punctuation\n",
    "            if token in ['.', ',', '!', '?', ';', ':']:\n",
    "                article_text[-1] += token\n",
    "            elif token in ['-', '/']:\n",
    "                article_text[-1] += token\n",
    "            else:\n",
    "                # Add a space before the token if it's not the start of the article or after a newline\n",
    "                if len(article_text) > 0 and article_text[-1] != \"\\n\":\n",
    "                    article_text.append(\" \")\n",
    "                article_text.append(token)\n",
    "        \n",
    "        # Process location entities\n",
    "        if ner == \"B-LOC\":\n",
    "            if current_location:\n",
    "                toponyms.append(\" \".join(current_location))\n",
    "            current_location = [token]\n",
    "        elif ner == \"I-LOC\" and current_location:\n",
    "            current_location.append(token)\n",
    "        else:\n",
    "            if current_location:\n",
    "                toponyms.append(\" \".join(current_location))\n",
    "                current_location = []\n",
    "\n",
    "    # Add the last article if it exists\n",
    "    if article_text:\n",
    "        if current_location:\n",
    "            toponyms.append(\" \".join(current_location))\n",
    "        dataset.append({\n",
    "            'text_id': text_id,\n",
    "            'text': \"\".join(article_text),\n",
    "            'toponyms': toponyms\n",
    "        })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def save_as_json(data, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Path to the CoNLL-2003 English dataset\n",
    "conll_file_path = '../data/fine_tuning_data/CoNLL_train.txt'\n",
    "\n",
    "# Parsing the dataset\n",
    "conll_lines = parse_conll2003_file(conll_file_path)\n",
    "\n",
    "# Processing the data to extract text and location entities\n",
    "processed_data = process_conll2003_data(conll_lines)\n",
    "\n",
    "# Saving the output as a JSON file\n",
    "output_file_path = 'conll2003_location_entities.json'\n",
    "save_as_json(processed_data, output_file_path)\n",
    "\n",
    "print(f\"Processed data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in tqdm(processed_data):\n",
    "    \n",
    "    toponyms = article['toponyms']\n",
    "    response = {\"toponyms\":list(set(toponyms))}\n",
    "    input = article['text']\n",
    "    ft_data.append({\"instruction\":geoparse_instruction,\n",
    "                    \"input\":input,\n",
    "                    \"response\":str(response)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ft_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fine_tuning_data/llama3_toponym_extraction_ft.json', 'w') as f:\n",
    "    json.dump(ft_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ft_data[-24]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-geoparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
