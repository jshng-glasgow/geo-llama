{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg1cjt5oAxyi",
        "outputId": "eef4971c-2c81-4b87-fee5-212bc767a86c"
      },
      "outputs": [],
      "source": [
        "# standard library imports\n",
        "import json\n",
        "import sys\n",
        "# third party imports\n",
        "from geopy.distance import distance\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "# local imports\n",
        "sys.path.append('..')\n",
        "from geo_llama.main import GeoLlama\n",
        "from geo_llama.model import TopoModel, RAGModel\n",
        "from geo_llama.translator import Translator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_fLgp4nrsMI"
      },
      "source": [
        "# Testing GeoLlama on foreign language text\n",
        "In this notebook we will test the model on text from non-english sources. We have 15 texts in French, Traditional Chinese and Simplified Chinese.\n",
        "\n",
        "We will test the model without the initial translation stage first, since Llama3 is desinged to have multi-lingual capability. The results will be compared against human annotated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkUEg_LcEVyX",
        "outputId": "e01f4ed5-224a-454b-c051-3b56ddc9e6bb"
      },
      "outputs": [],
      "source": [
        "# set up the model\n",
        "topo_model = TopoModel(model_name='JoeShingleton/GeoLlama_7b_toponym',\n",
        "                       prompt_path='../data/prompt_templates/prompt_template.txt',\n",
        "                       instruct_path='../data/prompt_templates/topo_instruction.txt',\n",
        "                       input_path=None,\n",
        "                       config_path='../data/config_files/model_config.json')\n",
        "\n",
        "rag_model = RAGModel(model_name='JoeShingleton/GeoLlama_7b_RAG',\n",
        "                       prompt_path='../data/prompt_templates/prompt_template.txt',\n",
        "                       instruct_path='../data/prompt_templates/rag_instruction.txt',\n",
        "                       input_path='../data/prompt_templates/rag_input.txt',\n",
        "                       config_path='../data/config_files/model_config.json')\n",
        "\n",
        "translator = Translator(model_size=\"1.2B\")\n",
        "\n",
        "geo_llama = GeoLlama(topo_model, rag_model, translate_model=translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61dwfqx5E3eF",
        "outputId": "9b5981ce-3371-41f3-8151-c22fc8f59e86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [06:06<00:00, 24.44s/it]\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "for article in tqdm(range(1,16)):\n",
        "  path = '../data/test_data/non_english_texts/article'+str(article)+'.txt'\n",
        "  with open(path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "  results.append(geo_llama.geoparse(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "Yyg5ASsWvfpE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from geopy.distance import distance\n",
        "\n",
        "true_data = pd.read_csv('../data/test_data/translated_toponyms.txt', encoding='utf-16', delimiter='\\t')\n",
        "\n",
        "correct_distance = []\n",
        "precision = []\n",
        "recall = []\n",
        "for i, pred_toponyms in enumerate(results):\n",
        "  true_toponyms = true_data[true_data.Article==i+1]\n",
        "  correct_toponyms = []\n",
        "  for pred in pred_toponyms:\n",
        "    if pred['name'] in true_toponyms.Toponym.values:\n",
        "      correct_toponyms.append(pred['name'])\n",
        "      correct_location = (pred['latitude'], pred['longitude'])\n",
        "      true_toponym  = true_toponyms[true_toponyms.Toponym==pred['name']]\n",
        "      true_location = (true_toponym.Latitude.values[0], true_toponym.Longitude.values[0])\n",
        "      correct_distance.append(distance(correct_location, true_location).km)\n",
        "  TP = len(correct_toponyms)\n",
        "  FP = len(pred_toponyms)-TP\n",
        "  FN = len(true_toponyms)-TP\n",
        "  if TP+FP==0:\n",
        "    precision.append(0)\n",
        "  else:\n",
        "    precision.append(TP/(TP+FP))\n",
        "  if TP+FN==0:\n",
        "    recall.append(0)\n",
        "  else:\n",
        "    recall.append(TP/(TP+FN))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Caa1Q8saFLUn",
        "outputId": "c0904a00-5231-4b5a-ea8b-fb3a93770e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro precision: 0.733\n",
            "Macro recall: 0.794\n",
            "Macro F1: 0.762\n"
          ]
        }
      ],
      "source": [
        "macro_precision = np.mean(precision)\n",
        "macro_recall = np.mean(recall)\n",
        "macro_f1 = 2*macro_precision*macro_recall/(macro_precision+macro_recall)\n",
        "print(f'Macro precision: {macro_precision:.3f}')\n",
        "print(f'Macro recall: {macro_recall:.3f}')\n",
        "print(f'Macro F1: {macro_f1:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgNi1fyDSv8g",
        "outputId": "daa2652e-ac11-459d-b5c1-49443f012496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc@1km: 0.640\n",
            "Acc@80km: 0.880\n",
            "Acc@161km: 0.960\n",
            "Mean distance: 21.217\n",
            "Median distance: 0.034\n"
          ]
        }
      ],
      "source": [
        "acc_at_1km = len([d for d in correct_distance if d<=1])/len(correct_distance)\n",
        "acc_at_80km = len([d for d in correct_distance if d<=80])/len(correct_distance)\n",
        "acc_at_161km = len([d for d in correct_distance if d<=161])/len(correct_distance)\n",
        "mean_distance = np.mean(correct_distance)\n",
        "median_distance = np.median(correct_distance)\n",
        "\n",
        "print(f'Acc@1km: {acc_at_1km:.3f}')\n",
        "print(f'Acc@80km: {acc_at_80km:.3f}')\n",
        "print(f'Acc@161km: {acc_at_161km:.3f}')\n",
        "print(f'Mean distance: {mean_distance:.3f}')\n",
        "print(f'Median distance: {median_distance:.3f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFhQM6-hA9fl",
        "outputId": "814211f1-238d-451f-cd02-9ab726500c47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 17 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:44<11:56, 44.75s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [01:29<11:09, 44.60s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [02:32<12:21, 52.96s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [02:59<09:16, 42.82s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [03:07<06:04, 30.41s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [03:39<05:38, 30.75s/it]\u001b[A\n",
            " 41%|████      | 7/17 [04:12<05:15, 31.52s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [04:48<04:57, 33.00s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [05:27<04:39, 34.89s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [05:48<03:34, 30.65s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [06:45<03:52, 38.71s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [07:30<03:22, 40.49s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [08:03<02:32, 38.19s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [08:26<01:41, 33.77s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [09:04<01:10, 35.03s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [09:33<00:33, 33.16s/it]\u001b[A\n",
            "100%|██████████| 17/17 [10:24<00:00, 36.73s/it]\n",
            "  7%|▋         | 1/15 [10:39<2:29:06, 639.07s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 7 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 14%|█▍        | 1/7 [00:34<03:25, 34.19s/it]\u001b[A\n",
            " 29%|██▊       | 2/7 [02:02<05:29, 65.88s/it]\u001b[A\n",
            " 43%|████▎     | 3/7 [03:10<04:27, 66.88s/it]\u001b[A\n",
            " 57%|█████▋    | 4/7 [04:25<03:30, 70.02s/it]\u001b[A\n",
            " 71%|███████▏  | 5/7 [05:22<02:10, 65.44s/it]\u001b[A\n",
            " 86%|████████▌ | 6/7 [06:07<00:58, 58.60s/it]\u001b[A\n",
            "100%|██████████| 7/7 [06:34<00:00, 56.34s/it]\n",
            " 13%|█▎        | 2/15 [17:30<1:49:24, 504.95s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 9 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 1/9 [00:38<05:09, 38.67s/it]\u001b[A\n",
            " 22%|██▏       | 2/9 [01:41<06:08, 52.65s/it]\u001b[A\n",
            " 33%|███▎      | 3/9 [02:47<05:53, 58.93s/it]\u001b[A\n",
            " 44%|████▍     | 4/9 [03:14<03:52, 46.44s/it]\u001b[A\n",
            " 56%|█████▌    | 5/9 [03:46<02:44, 41.23s/it]\u001b[A\n",
            " 67%|██████▋   | 6/9 [04:20<01:55, 38.56s/it]\u001b[A\n",
            " 78%|███████▊  | 7/9 [05:06<01:21, 40.95s/it]\u001b[A\n",
            " 89%|████████▉ | 8/9 [05:55<00:43, 43.68s/it]\u001b[A\n",
            "100%|██████████| 9/9 [06:42<00:00, 44.72s/it]\n",
            " 20%|██        | 3/15 [24:53<1:35:21, 476.76s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 6 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|█▋        | 1/6 [00:27<02:15, 27.17s/it]\u001b[A\n",
            " 33%|███▎      | 2/6 [01:28<03:08, 47.10s/it]\u001b[A\n",
            " 50%|█████     | 3/6 [02:09<02:13, 44.43s/it]\u001b[A\n",
            " 67%|██████▋   | 4/6 [03:30<01:57, 58.66s/it]\u001b[A\n",
            " 83%|████████▎ | 5/6 [04:12<00:52, 52.81s/it]\u001b[A\n",
            "100%|██████████| 6/6 [04:58<00:00, 49.81s/it]\n",
            " 27%|██▋       | 4/15 [30:15<1:16:12, 415.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 4 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25%|██▌       | 1/4 [01:13<03:39, 73.25s/it]\u001b[A\n",
            " 50%|█████     | 2/4 [01:56<01:50, 55.29s/it]\u001b[A\n",
            " 75%|███████▌  | 3/4 [03:00<00:59, 59.35s/it]\u001b[A\n",
            "100%|██████████| 4/4 [04:24<00:00, 66.13s/it]\n",
            " 33%|███▎      | 5/15 [35:01<1:01:30, 369.03s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 8 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 12%|█▎        | 1/8 [01:54<13:19, 114.15s/it]\u001b[A\n",
            " 25%|██▌       | 2/8 [02:01<05:08, 51.39s/it] \u001b[A\n",
            " 38%|███▊      | 3/8 [04:13<07:19, 88.00s/it]\u001b[A\n",
            " 50%|█████     | 4/8 [05:47<06:01, 90.33s/it]\u001b[A\n",
            " 62%|██████▎   | 5/8 [06:33<03:43, 74.36s/it]\u001b[A\n",
            " 75%|███████▌  | 6/8 [06:42<01:44, 52.24s/it]\u001b[A\n",
            " 88%|████████▊ | 7/8 [07:46<00:56, 56.02s/it]\u001b[A\n",
            "100%|██████████| 8/8 [09:21<00:00, 70.23s/it]\n",
            " 40%|████      | 6/15 [44:29<1:05:28, 436.52s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 7 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 14%|█▍        | 1/7 [00:34<03:24, 34.04s/it]\u001b[A\n",
            " 29%|██▊       | 2/7 [01:44<04:37, 55.51s/it]\u001b[A\n",
            " 43%|████▎     | 3/7 [02:47<03:56, 59.04s/it]\u001b[A\n",
            " 57%|█████▋    | 4/7 [04:07<03:21, 67.33s/it]\u001b[A\n",
            " 71%|███████▏  | 5/7 [05:01<02:04, 62.36s/it]\u001b[A\n",
            " 86%|████████▌ | 6/7 [07:06<01:23, 83.79s/it]\u001b[A\n",
            "100%|██████████| 7/7 [08:08<00:00, 69.83s/it]\n",
            " 47%|████▋     | 7/15 [53:11<1:01:57, 464.66s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 6 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|█▋        | 1/6 [00:41<03:23, 40.74s/it]\u001b[A\n",
            " 33%|███▎      | 2/6 [01:21<02:43, 40.92s/it]\u001b[A\n",
            " 50%|█████     | 3/6 [02:28<02:37, 52.62s/it]\u001b[A\n",
            " 67%|██████▋   | 4/6 [02:44<01:16, 38.05s/it]\u001b[A\n",
            " 83%|████████▎ | 5/6 [03:38<00:44, 44.08s/it]\u001b[A\n",
            "100%|██████████| 6/6 [04:10<00:00, 41.73s/it]\n",
            " 53%|█████▎    | 8/15 [57:57<47:34, 407.81s/it]  /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 13 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 1/13 [00:49<09:50, 49.22s/it]\u001b[A\n",
            " 15%|█▌        | 2/13 [02:06<12:04, 65.85s/it]\u001b[A\n",
            " 23%|██▎       | 3/13 [03:24<11:51, 71.11s/it]\u001b[A\n",
            " 31%|███       | 4/13 [03:56<08:21, 55.67s/it]\u001b[A\n",
            " 38%|███▊      | 5/13 [04:51<07:22, 55.30s/it]\u001b[A\n",
            " 46%|████▌     | 6/13 [05:20<05:26, 46.70s/it]\u001b[A\n",
            " 54%|█████▍    | 7/13 [06:20<05:04, 50.78s/it]\u001b[A\n",
            " 62%|██████▏   | 8/13 [07:05<04:06, 49.26s/it]\u001b[A\n",
            " 69%|██████▉   | 9/13 [07:28<02:43, 40.80s/it]\u001b[A\n",
            " 77%|███████▋  | 10/13 [07:46<01:42, 34.02s/it]\u001b[A\n",
            " 85%|████████▍ | 11/13 [08:31<01:14, 37.27s/it]\u001b[A\n",
            " 92%|█████████▏| 12/13 [09:18<00:40, 40.32s/it]\u001b[A\n",
            "100%|██████████| 13/13 [10:18<00:00, 47.55s/it]\n",
            " 60%|██████    | 9/15 [1:09:02<48:47, 487.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 4 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25%|██▌       | 1/4 [00:21<01:05, 21.72s/it]\u001b[A\n",
            " 50%|█████     | 2/4 [01:19<01:26, 43.04s/it]\u001b[A\n",
            " 75%|███████▌  | 3/4 [01:41<00:33, 33.53s/it]\u001b[A\n",
            "100%|██████████| 4/4 [02:44<00:00, 41.25s/it]\n",
            " 67%|██████▋   | 10/15 [1:12:15<33:05, 397.09s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 5 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:27<01:50, 27.73s/it]\u001b[A\n",
            " 40%|████      | 2/5 [00:44<01:04, 21.51s/it]\u001b[A\n",
            " 60%|██████    | 3/5 [01:18<00:53, 26.88s/it]\u001b[A\n",
            " 80%|████████  | 4/5 [01:51<00:29, 29.54s/it]\u001b[A\n",
            "100%|██████████| 5/5 [02:19<00:00, 27.90s/it]\n",
            " 73%|███████▎  | 11/15 [1:14:40<21:19, 319.82s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 4 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25%|██▌       | 1/4 [01:28<04:25, 88.53s/it]\u001b[A\n",
            " 50%|█████     | 2/4 [02:02<01:53, 56.68s/it]\u001b[A\n",
            " 75%|███████▌  | 3/4 [02:35<00:45, 45.53s/it]\u001b[A\n",
            "100%|██████████| 4/4 [04:25<00:00, 66.47s/it]\n",
            " 80%|████████  | 12/15 [1:19:19<15:22, 307.53s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 13 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 1/13 [00:26<05:14, 26.19s/it]\u001b[A\n",
            " 15%|█▌        | 2/13 [01:09<06:35, 35.92s/it]\u001b[A\n",
            " 23%|██▎       | 3/13 [01:49<06:17, 37.80s/it]\u001b[A\n",
            " 31%|███       | 4/13 [01:57<03:56, 26.23s/it]\u001b[A\n",
            " 38%|███▊      | 5/13 [02:36<04:06, 30.77s/it]\u001b[A\n",
            " 46%|████▌     | 6/13 [03:39<04:50, 41.46s/it]\u001b[A\n",
            " 54%|█████▍    | 7/13 [03:56<03:22, 33.71s/it]\u001b[A\n",
            " 62%|██████▏   | 8/13 [04:54<03:27, 41.57s/it]\u001b[A\n",
            " 69%|██████▉   | 9/13 [05:22<02:28, 37.18s/it]\u001b[A\n",
            " 77%|███████▋  | 10/13 [06:12<02:02, 40.97s/it]\u001b[A\n",
            " 85%|████████▍ | 11/13 [06:55<01:23, 41.96s/it]\u001b[A\n",
            " 92%|█████████▏| 12/13 [07:02<00:31, 31.32s/it]\u001b[A\n",
            "100%|██████████| 13/13 [07:43<00:00, 35.63s/it]\n",
            " 87%|████████▋ | 13/15 [1:27:17<11:57, 358.95s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 18 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/18 [00:36<10:18, 36.40s/it]\u001b[A\n",
            " 11%|█         | 2/18 [01:09<09:07, 34.25s/it]\u001b[A\n",
            " 17%|█▋        | 3/18 [01:32<07:21, 29.46s/it]\u001b[A\n",
            " 22%|██▏       | 4/18 [02:09<07:33, 32.36s/it]\u001b[A\n",
            " 28%|██▊       | 5/18 [02:26<05:45, 26.55s/it]\u001b[A\n",
            " 33%|███▎      | 6/18 [02:36<04:14, 21.20s/it]\u001b[A\n",
            " 39%|███▉      | 7/18 [03:09<04:36, 25.11s/it]\u001b[A\n",
            " 44%|████▍     | 8/18 [03:37<04:19, 25.97s/it]\u001b[A\n",
            " 50%|█████     | 9/18 [04:15<04:26, 29.58s/it]\u001b[A\n",
            " 56%|█████▌    | 10/18 [04:39<03:41, 27.74s/it]\u001b[A\n",
            " 61%|██████    | 11/18 [05:09<03:19, 28.57s/it]\u001b[A\n",
            " 67%|██████▋   | 12/18 [05:21<02:21, 23.56s/it]\u001b[A\n",
            " 72%|███████▏  | 13/18 [05:57<02:17, 27.40s/it]\u001b[A\n",
            " 78%|███████▊  | 14/18 [06:23<01:47, 26.78s/it]\u001b[A\n",
            " 83%|████████▎ | 15/18 [07:04<01:33, 31.19s/it]\u001b[A\n",
            " 89%|████████▉ | 16/18 [07:25<00:56, 28.29s/it]\u001b[A\n",
            " 94%|█████████▍| 17/18 [08:02<00:30, 30.66s/it]\u001b[A\n",
            "100%|██████████| 18/18 [08:27<00:00, 28.19s/it]\n",
            " 93%|█████████▎| 14/15 [1:37:14<07:11, 431.13s/it]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating 7 lines:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 14%|█▍        | 1/7 [00:25<02:33, 25.62s/it]\u001b[A\n",
            " 29%|██▊       | 2/7 [01:02<02:42, 32.47s/it]\u001b[A\n",
            " 43%|████▎     | 3/7 [01:17<01:37, 24.41s/it]\u001b[A\n",
            " 57%|█████▋    | 4/7 [01:49<01:21, 27.30s/it]\u001b[A\n",
            " 71%|███████▏  | 5/7 [02:30<01:04, 32.13s/it]\u001b[A\n",
            " 86%|████████▌ | 6/7 [02:53<00:29, 29.07s/it]\u001b[A\n",
            "100%|██████████| 7/7 [03:40<00:00, 31.49s/it]\n",
            "100%|██████████| 15/15 [1:41:17<00:00, 405.17s/it]\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "translated_texts = []\n",
        "for article in tqdm(range(1,16)):\n",
        "  path = '../data/test_data/non_english_texts/article'+str(article)+'.txt'\n",
        "  with open(path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "  translated_text = translator.translate(text)\n",
        "  results.append(geo_llama.geoparse(translated_text['translation']))\n",
        "  translated_texts.append(translated_text['translation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kxbDsWiY9sPr"
      },
      "outputs": [],
      "source": [
        "with open('../data/results/translated_results.json', 'w') as f:\n",
        "  json.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hGNe_5YWBziL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from geopy.distance import distance\n",
        "\n",
        "true_data = pd.read_csv('../data/test_data/translated_toponyms.txt', encoding='utf-16', delimiter='\\t')\n",
        "\n",
        "correct_distance = []\n",
        "precision = []\n",
        "recall = []\n",
        "for i, pred_toponyms in enumerate(results):\n",
        "  true_toponyms = true_data[true_data.Article==i+1]\n",
        "  correct_toponyms = []\n",
        "  for pred in pred_toponyms:\n",
        "    if pred['name'] in true_toponyms.toponym_en.values:\n",
        "      correct_toponyms.append(pred['name'])\n",
        "      correct_location = (pred['latitude'], pred['longitude'])\n",
        "      true_toponym  = true_toponyms[true_toponyms.toponym_en==pred['name']]\n",
        "      true_location = (true_toponym.Latitude.values[0], true_toponym.Longitude.values[0])\n",
        "      correct_distance.append(distance(correct_location, true_location).km)\n",
        "  TP = len(correct_toponyms)\n",
        "  FP = len(pred_toponyms)-TP\n",
        "  FN = len(true_toponyms)-TP\n",
        "  if TP+FP==0:\n",
        "    precision.append(0)\n",
        "  else:\n",
        "    precision.append(TP/(TP+FP))\n",
        "  if TP+FN==0:\n",
        "    recall.append(0)\n",
        "  else:\n",
        "    recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5QgAdthler6",
        "outputId": "428f3904-2040-4253-a845-474903e2e957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro precision: 0.513\n",
            "Macro recall: 0.664\n",
            "Macro F1: 0.579\n"
          ]
        }
      ],
      "source": [
        "macro_precision = np.mean(precision)\n",
        "macro_recall = np.mean(recall)\n",
        "macro_f1 = 2*macro_precision*macro_recall/(macro_precision+macro_recall)\n",
        "print(f'Macro precision: {macro_precision:.3f}')\n",
        "print(f'Macro recall: {macro_recall:.3f}')\n",
        "print(f'Macro F1: {macro_f1:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_LtF6eFlmDB",
        "outputId": "cff7633c-1d50-4823-d82c-73712d22b2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc@1km: 0.625\n",
            "Acc@80km: 0.875\n",
            "Acc@161km: 0.925\n",
            "Mean distance: 270.736\n",
            "Median distance: 0.034\n"
          ]
        }
      ],
      "source": [
        "acc_at_1km = len([d for d in correct_distance if d<=1])/len(correct_distance)\n",
        "acc_at_80km = len([d for d in correct_distance if d<=80])/len(correct_distance)\n",
        "acc_at_161km = len([d for d in correct_distance if d<=161])/len(correct_distance)\n",
        "mean_distance = np.mean(correct_distance)\n",
        "median_distance = np.median(correct_distance)\n",
        "\n",
        "print(f'Acc@1km: {acc_at_1km:.3f}')\n",
        "print(f'Acc@80km: {acc_at_80km:.3f}')\n",
        "print(f'Acc@161km: {acc_at_161km:.3f}')\n",
        "print(f'Mean distance: {mean_distance:.3f}')\n",
        "print(f'Median distance: {median_distance:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assessing the translated toponyms\n",
        "There are likely some instances where a translated toponym is correct, but the traslations used in the ground truth is different. For example the ground truth might have the lable \"Unite States\" and the model might predict \"U.S.\", or the ground truth might be \"Hubei\" and the label \"Hubei Province\". We do not want to unduly penalise the model in these instances, so we will manually assess the accuracy of the model against he ground truth. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8QC0ZQyq9k-X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "with open('../data/results/translated_results.json', 'r') as f:\n",
        "    pred_data = json.load(f)\n",
        "    \n",
        "true_data = pd.read_csv('../data/test_data/translated_toponyms.txt', encoding='utf-16', delimiter='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "precision = []\n",
        "recall = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['Wuhan', 'China', 'Hubei']\n",
            "True toponyms = ['Wuhan']\n"
          ]
        }
      ],
      "source": [
        "idx = 0\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['Whuan'])\n",
        "FP = len(['China', 'Hubei'])\n",
        "FN = len([])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['China', 'U.S.', 'New York', 'United States']\n",
            "True toponyms = ['United States' 'China' 'New York']\n"
          ]
        }
      ],
      "source": [
        "idx = 1\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['China', 'U.S.', 'New York'])\n",
        "FP = len([])\n",
        "FN = len([])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['Laos', 'China', 'Ukraine', 'Russia', 'U.S.', 'San Francisco', 'Russian', 'United States', 'Russian military', 'Laos']\n",
            "True toponyms = ['United States' 'China' 'Vientiane' 'San Francisco' 'Laos' 'Russia'\n",
            " 'Ukraine']\n"
          ]
        }
      ],
      "source": [
        "idx = 2\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['Laos', 'China', 'Ukraine', 'Russia', 'U.S.', 'San Francisco'])\n",
        "FP = len(['Russian', 'Russian millitary'])\n",
        "FN = len(['Vientiane'])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['China', 'Hainan', 'Macao', 'Hong Kong', 'Hong Kong-Macao']\n",
            "True toponyms = ['Macao' 'Hong Kong' 'Hainan']\n"
          ]
        }
      ],
      "source": [
        "idx = 3\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['China', 'Hainan', 'Macao', 'Hong Kong'])\n",
        "FP = len(['Hong Kong-Macao'])\n",
        "FN = len([])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['Senna River', 'Paris', 'France', 'Manna River', 'Seine']\n",
            "True toponyms = ['Marne River' 'France' 'Paris' 'Seine River']\n"
          ]
        }
      ],
      "source": [
        "idx = 4\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "# As 'Senna River' and 'Manna River' are unlikely to be geolocated to the correct location, they will be considered incorrect\n",
        "TP = len(['Paris', 'France', 'Seine'])\n",
        "FP = len(['Senna River', 'Manna River'])\n",
        "FN = len([])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['Hong Kong']\n",
            "True toponyms = ['The Chinese University of Hong Kong']\n"
          ]
        }
      ],
      "source": [
        "idx = 5\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hongkong is within the toponym 'Hong Kong University...' so is correct\n",
        "TP = len([])\n",
        "FP = len(['Hong Kong'])\n",
        "FN = len(['The Chinese University of Hong Kong'])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['China', 'Hong Kong', 'Fujian Province', 'China', 'Tianjin City', 'Xiamen', 'Tianjin']\n",
            "True toponyms = ['Xiamen City' 'Fujian Province' 'China' 'Hong Kong' 'South China Sea'\n",
            " 'Ludao' 'Wulao Peak']\n"
          ]
        }
      ],
      "source": [
        "idx = 6\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['China', 'Hong Kong', 'Fujian Province', 'Xiamen'])\n",
        "FP = len(['Tianjin'])\n",
        "FN = len(['South China Sea', 'Wulao Peak', 'Ludao'])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['Cuba', 'U.S.', 'Nicaragua', 'Panama', 'Venezuela', 'Uruguay', 'Argentina', 'Honduras', 'Bolivia']\n",
            "True toponyms = ['Uruguay' 'Honduras' 'United States' 'Venezuela' 'China' 'Cuba'\n",
            " 'Nicaragua' 'Bolivia' 'Argentina' 'Panama']\n"
          ]
        }
      ],
      "source": [
        "idx = 7\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['Cuba', 'U.S.', 'Nicaragua', 'Panama', 'Venezuela', 'Uruguay', 'Argentina', 'Honduras', 'Bolivia'])\n",
        "FP = len([])\n",
        "FN = len(['China'])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['Osaka', 'Japan', 'Osaka City', 'Guangzhou', 'Great Britain', 'Osaka Dreamland World', 'Guangxi', 'San Diego', 'Osaka Guangxi', 'Japan']\n",
            "True toponyms = ['Kizugawa City' 'Japan' 'Stonehenge' 'United Kingdom' 'China' 'Kinki'\n",
            " 'Osaka Castle' 'Kizugawa City' 'Kyoto']\n"
          ]
        }
      ],
      "source": [
        "idx = 8\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['Osaka', 'Japan', 'UK'])\n",
        "FP = len(['Guanzhou', 'Osaka Dreamland Park', 'Guangxi', 'San Diego', 'Osaka Guangxi'])\n",
        "FN = len(['Kizugawa city', 'Stonehenge', 'Kinki', 'Kyoto'])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['Aikido County', '岐阜', 'Japan', 'County City', 'UNESCO', 'Tailand', 'UN']\n",
            "True toponyms = ['Gushi City' 'Gifu Prefecture' 'Japan' 'Ichinomiya City'\n",
            " 'Aichi Prefecture']\n"
          ]
        }
      ],
      "source": [
        "idx = 9\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['Japan'])\n",
        "FP = len(['Aikido County', '岐阜', 'County City', 'UNESCO', 'Tailand', 'UN'])\n",
        "FN = len(['Gushi City', 'Gifu Prefecture', 'Ichinomiya City', 'Aichi Prefecture'])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['California']\n",
            "True toponyms = ['California' 'San Francisco']\n"
          ]
        }
      ],
      "source": [
        "idx = 10\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['California'])\n",
        "FP = len([])\n",
        "FN = len(['San Francisco'])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['France', 'Seine-Maritime', 'Rouen']\n",
            "True toponyms = ['Seine-Maritime' 'Rouen']\n"
          ]
        }
      ],
      "source": [
        "idx = 11\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['Seine-Maritime', 'Rouen'])\n",
        "FP = len(['France'])\n",
        "FN = len([])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['Morocco', 'Tiflet', 'Rabat']\n",
            "True toponyms = ['Morocco' 'Tiflet' 'Rabat']\n"
          ]
        }
      ],
      "source": [
        "idx = 12\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['Morocco', 'Tiflet', 'Rabat'])\n",
        "FP = len([])\n",
        "FN = len([])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['U.S.', 'South China Sea', 'U.S. Secretary', 'Asia-Pacific', 'Moscow', 'Australia', 'China', 'Tokyo', 'Philippines', 'Japan', 'Indian Ocean', 'Manila', 'North Korea', 'Pacific', 'Beijing', 'Russia', 'Second Thomas', 'Palaos', 'Indo-Pacific', 'Indian', 'United States']\n",
            "True toponyms = [nan 'United States' 'Japan' 'Australia' 'Beijing' 'China' 'North Korea'\n",
            " 'Russia' 'Moscow' 'Ukraine' 'Palau' 'Manilla' 'India' 'Tokyo']\n"
          ]
        }
      ],
      "source": [
        "idx = 13\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['U.S.', 'Asia-Pacific', 'Moscow', 'Australia', 'China', 'Tokyo', 'Japan', 'Manila', 'North Korea', 'Beijing', 'Russia', 'Palaos', 'Indian'])\n",
        "FP = len(['South China Sea', 'U.S. Secretary', 'Philippines', 'Indian Ocean', 'Pacific', 'Second Thomas', 'Indo-Pacific'])\n",
        "FN = len([])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_toponyms ['Paris', 'Beirut', 'Israel', 'Middle East', 'Lebanon']\n",
            "True toponyms = ['Beirut' 'Lebanon' 'Charles-de-Gaulle']\n"
          ]
        }
      ],
      "source": [
        "idx = 14\n",
        "true = true_data[true_data['Article']==idx+1]\n",
        "pred = pred_data[idx]\n",
        "\n",
        "print(f'Predicted_toponyms {[t['name'] for t in pred]}')\n",
        "print(f'True toponyms = {true.toponym_en.values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = len(['Paris', 'Beirut', 'Lebanon'])\n",
        "FP = len(['Israel', 'Middle East'])\n",
        "FN = len(['Charles-de-Gaulle'])\n",
        "\n",
        "precision.append(TP/(TP+FP))\n",
        "recall.append(TP/(TP+FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro precision: 0.648\n",
            "Macro recall: 0.747\n",
            "Macro F1: 0.694\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "macro_precision = np.mean(precision)\n",
        "macro_recall = np.mean(recall)\n",
        "macro_f1 = 2*macro_precision*macro_recall/(macro_precision+macro_recall)\n",
        "print(f'Macro precision: {macro_precision:.3f}')\n",
        "print(f'Macro recall: {macro_recall:.3f}')\n",
        "print(f'Macro F1: {macro_f1:.3f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
